% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classif.adaboost.R
\name{classif.adaboost}
\alias{classif.adaboost}
\title{Functional Classification using AdaBoost Algorithm}
\usage{
classif.adaboost(
  formula,
  data,
  classif = "classif.glm",
  par.classif,
  B = 100,
  alpha.boost = "Breiman",
  weights = "equal",
  verbose = FALSE,
  ...
)
}
\arguments{
\item{formula}{an object of class \code{formula} (or one that can be coerced
to that class): a symbolic description of the model to be fitted. The
details of model specification are given under \code{Details}.}

\item{data}{\code{list} that containing the variables in the model. 
The first element in the \code{data} list is called \emph{"df"} and it is a data
frame with the response and non functional explanatory variables, as \code{\link{glm}}. 
Functional covariates of class \code{fdata}  are introduced in the following items in the \code{data} list.}

\item{classif}{Name of classifier, by default \code{"classif.glm"} then \code{\link{classif.glm}} function is used.
Other  classifiers:  \code{\link{classif.gsam}}, \code{\link{classif.rpart}}, \code{\link{classif.nnet}} and \code{\link{classif.svm}}, etc.}

\item{par.classif}{List of parameters for the classifier.}

\item{B}{\code{integer}, maximum number of iterations.}

\item{alpha.boost}{\code{character}.   Three types: 
\itemize{
 \item Breiman (by default), \eqn{\alpha}=1/2 ln((1-err)/err)
 \item Freund, \eqn{\alpha}=ln((1-err)/err)
 \item Zhu, \eqn{\alpha}=ln((1-err)/err) + ln(nclasses-1)
}
where \code{err} is the misclassification error.}

\item{weights}{Initial weights:
\itemize{
 \item \code{character}, if \code{'equal'} (by default)  provides same weights for each observation; 
 if \code{'inverse'} provides inverse-probability of weighting.   
\item \code{numeric},  the user can provides a vector of length \code{n} with weigths for each observation.
}}

\item{verbose}{\code{logical}, if \code{TRUE} prints relevant information for each iteration.}

\item{\dots}{Further arguments passed to or from other methods.}
}
\value{
Return the same result of the classification method plus:
\itemize{
\item \code{group.est.boost}, \code{matrix} of \code{n X B},  the fitted response (by row) in each iteration (by column).
\item \code{alpha.boost}, \code{vector} of length \code{B} with the  \eqn{\alpha} coefficients.  
The predicted classes are a linear combination of all of the classifiers weighted by \code{alpha.boost}.
\item \code{votes.boost}, \code{matrix} of \code{n X g},  with   votes values (by row)  for each class group  (by column).
\item \code{prob.boost}, \code{matrix} of \code{n X g},   boosting probabilities of each observation (by row) for each class group  (by column).
\item \code{weigths.rep}, \code{matrix} of \code{n X B},  with  the weigths values (by row) in each iteration (by column).
\item \code{list.fit}, \code{list} with the \code{B} fitted models. 
}
}
\description{
Computes classification using functional (and non functional)
explanatory variables by AdaBoost algorithm.  
The ensamble classifier is  a linear combination of all of the (weak) classifiers
 using the \eqn{\alpha}{\alpha} weight updating coefficients.
}
\note{
Adapted version from \code{adabag::boosting} function.
}
\examples{
\dontrun{
data(phoneme)
mlearn <- phoneme[["learn"]]
glearn <- phoneme[["classlearn"]]
mtest <- phoneme[["test"]]
gtest <- phoneme[["classtest"]]
dataf <- data.frame(glearn)
# Estimation
ij <- c(41:70, 101:200, 241:250)
dat = list("df" = dataf[ij, , drop = F], "x" = mlearn[ij, ])
res1 <- classif.glm(glearn ~ x, data = dat)
res2 <- classif.adaboost(glearn ~ x,data = dat,
                   classif = "classif.glm", B = 25)
res1$max.prob;res2$max.prob
# Prediction
newdat = list("df" = dataf, "x" = mtest)
pred1 <- predict(res1, newdat)
pred2 <- predict(res2, newdat)
cat2meas(pred1, newdat$df$glearn); cat2meas(pred2, newdat$df$glearn)
diag(table(pred1, newdat$df$glearn))/50
diag(table(pred2, newdat$df$glearn))/50
}

}
\references{
Breiman, L. (1998): 'Arcing classifiers'. \emph{The Annals of Statistics}, Vol 26, 3, pp. 801-849.\cr
Freund, Y. and Schapire, R.E. (1996): "Experiments with a new boosting algorithm". In Proceedings of the Thirteenth International Conference on Machine Learning, pp. 148-156, Morgan
Kaufmann.

Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2009): 'Multi-class AdaBoost.  \emph{Statistics and Its
Interface}, 2, pp. 349-360.

Alfaro, E., Gamez, M. and Garcia, N. (2013): 'adabag: An R Package for Classification with
Boosting and Bagging'.  \emph{Journal of Statistical Software}, Vol 54, 2, pp. 1-35.
}
\seealso{
See Also as: \code{\link{classif.glm}},  \code{\link{classif.gsam}},  \code{\link{classif.svm}}
 and other classifiers.
}
\author{
Febrero-Bande, M. and Oviedo de la Fuente, M.
}
\keyword{classif}
