% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classif.bootstrap.R
\name{classif.bootstrap}
\alias{classif.bootstrap}
\alias{classif.bootstrap2}
\title{Functional Classification using Bootstrap}
\usage{
classif.bootstrap(
  formula,
  data,
  weights = "equal",
  B = 50,
  par.boot = list(nb = NULL, smo = 0, Nhull = NULL, Nnbh = NULL),
  classif = "classif.glm",
  par.classif,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{formula}{an object of class \code{formula} (or one that can be coerced
to that class): a symbolic description of the model to be fitted. The
details of model specification are given under \code{Details}.}

\item{data}{\code{list} that containing the variables in the model. The first item in the
\code{data} list is called \emph{"df"} and is a data frame with the response and non 
functional explanatory variables, as  \code{\link{glm}}.  Functional covariates of class 
\code{fdata} or \code{fd} are introduced in' the following items in the \code{data} list.}

\item{weights}{Initial weights:
\itemize{
 \item \code{character}, if \code{'equal'} (by default)  provides same weights for each observation; 
 if \code{'inverse'} provides inverse-probability of weighting.   
\item \code{numeric},  the user can provides a vector of length \code{n} with weigths for each observation.
}}

\item{B}{\code{integer}, number of boostrap resample,}

\item{par.boot, }{list with convex boostrap parameter, see \code{\link{group.bootstrap}} function for mor details.}

\item{classif}{Name of classifier, by default \code{"classif.glm"} then \code{\link{classif.glm}} function is used.
Other  classifiers:  \code{\link{classif.gsam}}, \code{\link{classif.rpart}}, \code{\link{classif.nnet}} and \code{\link{classif.svm}}, etc.}

\item{par.classif}{List of parameters for the classifier.}

\item{verbose}{\code{logical}, if \code{TRUE} print relevant information for each iteration.}

\item{\dots}{Further arguments passed to or from other methods.}

\item{boot}{\code{character}, type of boostrap resample,  Values are "global" and "local". See \code{\link{group.bootstrap}} function for mor details.}
}
\value{
Return a list of length sample\code{B}, tha corresond to the \code{B} fitted models. A convex bootstrap sample is generated to fit each 
 model, then the bootstrap re is compued. 
 The model fitted has the same  result of the classification method plus:\cr
\itemize{
 \item \code{predicted.values}, \code{vector} of size \code{n},  with the predicted response using the fitted model with  \code{nb} bootstrap sample.
 \item \code{alpha}, \code{numeric}, error rate value used to ensamble the \code{B} models. 
 \item \code{error}, \code{numeric}, the error values of missclassification  (by default is the accuracy).
}
}
\description{
Computes functional classification using convex and local convex bootstrap for functional (and non functional)
explanatory variables
}
\details{
The ensamble (final) classifier is  a linear combination of all of the \code{B} classifiers
using the \eqn{\alpha}{\alpha} coefficients.  Breiman coefficient \eqn{\alpha}=1/2 ln((1-err)/err) is computed for each classifier.
}
\examples{
\dontrun{
data(phoneme)
mlearn <- phoneme[["learn"]]
glearn <- phoneme[["classlearn"]]
mtest <- phoneme[["test"]]
gtest <- phoneme[["classtest"]]
dataf <- data.frame(glearn)
# Estimation
ij <- c(41:70, 101:200, 241:250)
dat = ldata("df" = dataf[ij, , drop = F], "x" = mlearn[ij, ])
res1 <- classif.glm(glearn ~ x, dat)
# Naive bootstrap
res2 <- classif.bootstrap(glearn ~ x, dat, classif = "classif.glm") 
# res2 <- classif.bootstrap(glearn ~ x, dat, classif = "classif.glm"
,par.boot=list("smo"=0)) 
# Smooth bootstrap
res3 <- classif.bootstrap(glearn ~ x, dat, classif = "classif.glm"
,par.boot=list("smo"=0.5)) 
# Global convex bootstrap
res4 <- classif.bootstrap(glearn ~ x, dat, classif = "classif.glm"
,par.boot=list("Nhull"=4)) 
# Local convex bootstrap
res5 <- classif.bootstrap(glearn ~ x, dat, classif = "classif.glm"
,par.boot=list("Nhull"=4,"Nnbh"=12)) 
# Prediction
newdat = list("df" = dataf, "x" = mtest)
pred1 <- predict(res1, newdat)
pred2 <- predict(res2, newdat)
pred3 <- predict(res3, newdat)
pred4 <- predict(res4, newdat)
pred5 <- predict(res5, newdat)
cat2meas(pred1, newdat$df$glearn)
cat2meas(pred2, newdat$df$glearn)
cat2meas(pred3, newdat$df$glearn)
cat2meas(pred4, newdat$df$glearn)
cat2meas(pred5, newdat$df$glearn)
diag(table(pred1, newdat$df$glearn))/50
diag(table(pred2, newdat$df$glearn))/50
diag(table(pred3, newdat$df$glearn))/50 
diag(table(pred4, newdat$df$glearn))/50
diag(table(pred5, newdat$df$glearn))/50  
}
}
\references{
Ramsay, James O., and Silverman, Bernard W. (2006), \emph{
Functional Data Analysis}, 2nd ed., Springer, New York. 

Breiman, L. (1998): 'Arcing classifiers'. \emph{The Annals of Statistics}, Vol 26, 3, pp. 801-849.
}
\seealso{
See Also as: \code{\link{classif.glm}},  \code{\link{classif.gsam}},  \code{\link{classif.svm}} and other classifiers.\cr Alternative method:
\code{\link{classif.adaboost}}.
}
\author{
Febrero-Bande, M. and Oviedo de la Fuente, M.
}
\keyword{classif}
